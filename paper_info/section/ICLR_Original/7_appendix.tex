\section*{Statement on the Use of Large Language Models}

In the interest of transparency and in compliance with the ICLR 2026 guidelines, we report that a large language model (LLM) was used to assist in the refinement of this paper's text.

\paragraph{Scope of Use.} 
The model's role was strictly limited to that of a writing assistant. Its contributions include:
\begin{itemize}
    \item Correcting grammatical errors, spelling, and punctuation.
    \item Improving sentence structure and flow for enhanced clarity.
    \item Refining word choices for greater precision and conciseness.
\end{itemize}




\input{images_tex/appendix_object_feature}
\subsection{Feature extraction}
\textbf{Object feature}
%우리는 object feature를 추출하기 위해 key frame의 Gaussian에 대해 velocity vector와 임베딩 파라미터를 활용한다. 각 key frame의 velocity vector는 CHS에 적용되어 baseline deformation과 함께 HOI module의 입력 feature로 공동 최적화된다. 추가적으로 각 key frame의 Gaussian마다 29차원의 learnable parameter를 임베딩하며, 이 임베딩은 velocity vector와 결합되어 feature로 사용된다. CHS를 통해 interpolation된 Gaussian feature는 결합된 feature와 time 정보를 입력으로 받아 얕은 MLP layer를 거쳐 projection되며, 최종적으로 32차원의 feature를 형성한다.
As shown in Fig. \textcolor{blue}{\ref{appendix_objfeature}}(a), we extract object features by leveraging the velocity vectors and embedding parameters of Gaussians at key frames. As shown in Fig. \textcolor{blue}{\ref{appendix_objfeature}}(b), each key frame’s velocity vector is applied to the CHS and jointly optimized with the baseline deformation as input features for the HOI module. In addition, a 29-dimensional learnable parameter is embedded for each key frame Gaussian, which is concatenated with the velocity vector to form the feature representation. The interpolated Gaussian features produced by CHS are then combined with the concatenated feature and time information, and projected through a shallow MLP, resulting in a 32-dimensional feature vector.



\textbf{Human feature}
Fig. \textcolor{blue}{~\ref{appendix_humanfeature}} illustrates the process of human feature extraction. 
We divide the SMPL-X model into 16 body parts and learn features corresponding to each part. 
Temporal features are sampled from the hexplane at SMPL-X vertices, where each feature at time $t$ is obtained based on the coordinates $(x_t, y_t, z_t)$. 
For each body part, the features of its associated vertices are averaged to form the part-specific representation $F_{\text{human}}$: 
\[
F_{\text{part}} = \frac{1}{N} \sum_{i \in \text{part}} f_i(x_t, y_t, z_t), \tag{10}
\]
where $N$ denotes the number of vertices belonging to the part. 
As a result, 16 part features, including head, torso, arms, and legs, are obtained and used as inputs to the HOI module. 
This design captures temporally varying dynamic representations while preserving semantically meaningful features for individual body parts.



\subsection{HOI module network detail}
As shown in Fig. \textcolor{blue}{\ref{appendix_HOINetwork}}, the proposed HOI module takes the time-varying features of humans and objects as inputs and explicitly models their interactions.
Let the human feature be denoted as $F_{\text{Human}} \in \mathbb{R}^{N_h \times d}$ and the object feature as $F_{\text{Object}} \in \mathbb{R}^{N_o \times d}$, 
where $N_h$ and $N_o$ are the numbers of feature tokens for human and object respectively, and $d$ is the feature dimension. 

To capture interdependencies between the two modalities, we apply a \textit{mutual-attention} mechanism. 
Specifically, queries ($Q$), keys ($K$), and values ($V$) are obtained via learnable linear projections:
\begin{equation}
Q_h = F_{\text{Human}}W_h^Q,\;\; K_o = F_{\text{Object}}W_o^K,\;\; V_o = F_{\text{Object}}W_o^V, \tag{11}
\end{equation}
\begin{equation}
Q_o = F_{\text{Object}}W_o^Q,\;\; K_h = F_{\text{Human}}W_h^K,\;\; V_h = F_{\text{Human}}W_h^V, \tag{12}
\end{equation}
where $W_h^Q, W_h^K, W_h^V, W_o^Q, W_o^K, W_o^V \in \mathbb{R}^{d \times d}$ are learnable projection matrices. 

Cross-attention is then computed in both directions: from human to object and from object to human. 
To enforce spatial priors, a distance mask $B \in \mathbb{R}^{N_h \times N_o}$ is added to the attention logits, where $B_{ij}$ encodes the relative distance between the $i$-th human token and the $j$-th object token. 
The resulting attention maps are defined as:
\begin{equation}
A_{h \leftarrow o} = \text{softmax}\!\left(\tfrac{Q_hK_o^\top}{\sqrt{d}} + B\right),\quad
A_{o \leftarrow h} = \text{softmax}\!\left(\tfrac{Q_oK_h^\top}{\sqrt{d}} + B^\top\right). \tag{13}
\end{equation}

Using these attention weights, the updated features are obtained as:
\begin{equation}
F'_{\text{Human}} = A_{h \leftarrow o} V_o,\quad
F'_{\text{Object}} = A_{o \leftarrow h} V_h. \tag{14}
\end{equation}

The updated human feature $F'_{\text{Human}}$ is then fed into a small MLP head to regress the refinement terms of SMPL-X parameters: 
\[
\Delta \text{SMPL-X} = \{\Delta \theta_{\text{body}},\; \Delta \theta_{\text{hand}},\; \Delta t\}, \tag{15}
\]
where $\Delta \theta_{\text{body}}$ and $\Delta \theta_{\text{hand}}$ denote pose corrections for body and hands, and $\Delta t$ is the global translation refinement. 
Similarly, the updated object feature $F'_{\text{Object}}$ is used to regress Gaussian-based object motion corrections:
\[
\Delta G_{\text{object}} \in \mathbb{R}^{N_o \times 3}, \tag{16}
\]
which represent displacement vectors applied to object Gaussians. 

In this way, the HOI module augments the baseline deformations (hexplane+LBS for humans and CHS for objects) with interaction-aware refinements, enabling accurate reconstruction of complex human--object interaction scenes.

\input{images_tex/appendix_human_feature}

\subsection{Objective Function Details}
The overall loss function of our model is defined as follows:

\begin{equation}
L = \gamma L_{\text{object motion}} + \beta L_{\text{human}} + \sigma L_{\text{scene}}, \tag{17}
\end{equation}

where $L_{\text{object motion}}$, $L_{\text{human}}$, and $L_{\text{scene}}$ correspond to losses for object motion, human modeling, and scene context, respectively. The weights $\gamma$, $\beta$, and $\sigma$ control the relative importance of each loss term and are specifically set to 1.0, 0.5, and 0.25, respectively. In our approach, these three terms are optimized simultaneously to consistently model the interactions between humans and objects.

\textbf{Human Loss details} \\
The $L_{\text{human}}$ term consists of losses related to human representation using the SMPL-X (\textcolor{blue}{\cite{pavlakos2019expressive}}) model. Specifically, it includes the reprojection error between the 3D human joint positions and detected 2D keypoints in images, a mesh-based face loss enhancing the consistency of facial geometry and texture, and a Laplacian regularization term. Additionally, there is an L1 loss ($L_{\text{smplx}}$) between the optimized SMPL-X parameters and the frame-wise initial SMPL-X parameters obtained by a regressor. These loss terms are directly adopted from previous methods such as ExAvatar (\textcolor{blue}{\cite{moon2024expressive}}), without modifications. For example, the face loss optimizes the consistency between rendered facial images and actual facial images, ensuring geometry-texture coherence. Laplacian regularization is applied to enhance the stability of human body shape. Further details can be found in the referenced research.

Formally, the human loss is given by:

\begin{equation}
L_{\text{human}} = L_{\text{kpt}} + L_{\text{face}} + L_{\text{reg}} + 0.1 \times L_{\text{smplx}}, \tag{18}
\end{equation}


\input{images_tex/appendix_HOINetwork}
\textbf{Scene Loss details}\\
The $L_{\text{scene}}$ term is a photometric loss focusing on the background regions of the entire scene, following the image similarity-based loss used in existing 3D Gaussian Splatting (\textcolor{blue}{\cite{kerbl20233d}}) (3DGS) methods. Specifically, a pre-trained human/object segmentation model is employed to mask out human and object regions in the images, optimizing the background Gaussians for the remaining pixels only. This involves minimizing the difference between the rendered result and the background pixels excluding the segmented human and object areas. Occlusions frequently occur during interactions between human hands and objects, causing inconsistencies in masks. By optimizing humans, objects, and backgrounds simultaneously, our method effectively mitigates these boundary inconsistencies.

The scene loss is explicitly defined as:

\begin{equation}
L_{\text{scene}} = 0.8 \times L_1(I_{\text{gt}}, I_{\text{render}}) 
+ 0.2 \times L_{\text{D-SSIM}}(I_{\text{gt}}, I_{\text{render}}), \tag{19}
\end{equation}

\textbf{Object Loss details} \\
The $L_{\text{object}}$ term is a photometric loss that focuses exclusively on the object regions within the scene. We render only the segmented object areas and compute the loss solely on these regions. A pre-trained object segmentation model is employed to isolate object masks in the input images. The object loss encourages accurate reconstruction and appearance consistency for moving objects, which often undergo significant deformation and motion. By supervising only the object regions, this loss helps to refine the geometry and texture of the object-specific Gaussians without being influenced by background or human-related elements.

The object loss is defined as:
\begin{equation}
L_{\text{object motion}} = 0.8 \times L_1(I_{\text{gt}}, I_{\text{obj}}) 
+ 0.2 \times L_{\text{D-SSIM}}(I_{\text{gt}}, I_{\text{obj}}). \tag{20}
\end{equation}